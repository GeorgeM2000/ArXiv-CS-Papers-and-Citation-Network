{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Libraries & Tools***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***PDF Extraction & Analysis***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Serial***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return io.BytesIO(response.content)\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
    "\n",
    "def extract_text_from_pdf(pdf_stream):\n",
    "    doc = fitz.open(\"pdf\", pdf_stream)  # Open PDF from memory\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"  # Extract text from each page\n",
    "    return text\n",
    "\n",
    "def extract_introduction(text, word_limit=400):\n",
    "    lines = text.split(\"\\n\")\n",
    "    introduction_found = False\n",
    "    extracted_text = []\n",
    "    word_count = 0\n",
    "\n",
    "    # Pattern for section title with numbering\n",
    "    section_pattern = re.compile(r'^\\s*(\\d+\\.|[IVXLCDM]+\\.|[A-Z]\\.)\\s+(INTRODUCTION|Introduction)\\s*$')\n",
    "\n",
    "    for line in lines:\n",
    "        if not introduction_found:\n",
    "            # Check if the line matches an Introduction section title\n",
    "            if section_pattern.match(line):\n",
    "                introduction_found = True\n",
    "        else:\n",
    "            # Split line into words and keep punctuation/symbols\n",
    "            words = line.split()\n",
    "            if word_count + len(words) > word_limit:\n",
    "                # Take only the words needed to reach the limit\n",
    "                words = words[: word_limit - word_count]\n",
    "                extracted_text.append(\" \".join(words))\n",
    "                break  # Stop once the limit is reached\n",
    "            else:\n",
    "                extracted_text.append(line)\n",
    "                word_count += len(words)\n",
    "\n",
    "    # Fix inline hyphenated words (e.g., \"poten- tially\" â†’ \"potentially\")\n",
    "    cleaned_text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', \" \".join(extracted_text))\n",
    "\n",
    "    return cleaned_text.strip() if cleaned_text else \"Introduction section not found.\"\n",
    "\n",
    "\n",
    "def process_pdf(url):\n",
    "    try:\n",
    "        pdf_stream = download_pdf(url)\n",
    "        #start_time = datetime.now()\n",
    "        text = extract_text_from_pdf(pdf_stream)\n",
    "        introduction_text = extract_introduction(text)\n",
    "        #end_time = datetime.now()\n",
    "        #print((end_time - start_time).total_seconds())\n",
    "        return introduction_text\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_url = \"https://arxiv.org/pdf/0704.1274\"  # Replace with your actual PDF URL\n",
    "intro_text = process_pdf(pdf_url)\n",
    "print(\"==================================================\")\n",
    "print(intro_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Parallel***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(paper_id):\n",
    "    url = f\"https://arxiv.org/pdf/{paper_id}\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return io.BytesIO(response.content)\n",
    "    else:\n",
    "        return None  # Indicate failure\n",
    "\n",
    "def extract_text_from_pdf(pdf_stream):\n",
    "    if not pdf_stream:\n",
    "        return None  # Failed to download\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(\"pdf\", pdf_stream)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "        return text if text.strip() else None  # Return None if empty\n",
    "    except:\n",
    "        return None  # Failed to extract text\n",
    "\n",
    "def extract_introduction(text, word_limit=400):\n",
    "    lines = text.split(\"\\n\")\n",
    "    introduction_found = False\n",
    "    extracted_text = []\n",
    "    word_count = 0\n",
    "    \n",
    "    section_pattern = re.compile(r'^\\s*(\\d+\\.|[IVXLCDM]+\\.|[A-Z]\\.)\\s+(INTRODUCTION|Introduction)\\s*$')\n",
    "    \n",
    "    for line in lines:\n",
    "        if not introduction_found:\n",
    "            if section_pattern.match(line):\n",
    "                introduction_found = True\n",
    "        else:\n",
    "            words = line.split()\n",
    "            if word_count + len(words) > word_limit:\n",
    "                words = words[: word_limit - word_count]\n",
    "                extracted_text.append(\" \".join(words))\n",
    "                break\n",
    "            else:\n",
    "                extracted_text.append(line)\n",
    "                word_count += len(words)\n",
    "    \n",
    "    cleaned_text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', \" \".join(extracted_text))\n",
    "    return cleaned_text.strip() if cleaned_text.strip() else None  # None if intro not found\n",
    "\n",
    "def process_range(paper_ids, start, end):\n",
    "    results = [(i, \"FAILED\") for i in range(start, end)]\n",
    "    failed_papers = [[] for _ in range(3)]  # Track failures: [download, extraction, introduction]\n",
    "    \n",
    "    for i, paper_id in enumerate(paper_ids, start=start):\n",
    "        pdf_stream = download_pdf(paper_id)\n",
    "        if not pdf_stream:\n",
    "            failed_papers[0].append((i, paper_id))\n",
    "            continue\n",
    "        \n",
    "        text = extract_text_from_pdf(pdf_stream)\n",
    "        if text is None:\n",
    "            failed_papers[1].append((i, paper_id))\n",
    "            continue\n",
    "        \n",
    "        introduction_text = extract_introduction(text)\n",
    "        if introduction_text is None:\n",
    "            failed_papers[2].append((i, paper_id))\n",
    "            continue\n",
    "        \n",
    "        results[i - start] = (i, introduction_text)\n",
    "    \n",
    "    return results, failed_papers\n",
    "\n",
    "def process_papers_parallel(id_file, ranges, output_file, max_workers=15):\n",
    "    with open(id_file, \"r\") as f:\n",
    "        paper_ids = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    failed_papers = [[] for _ in range(3)]  # Track failures: [download, extraction, introduction]\n",
    "    results = [\"FAILED\"] * len(paper_ids)  # Initialize output list with \"FAILED\"\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_range = {executor.submit(process_range, paper_ids[r[0]:r[1]], r[0], r[1]): r for r in ranges}\n",
    "        for future in future_to_range:\n",
    "            range_results, range_failures = future.result()\n",
    "            for i, text in range_results:\n",
    "                results[i] = text\n",
    "            for j in range(3):\n",
    "                failed_papers[j].extend(range_failures[j])\n",
    "    \n",
    "    with open(output_file, \"w\") as f:\n",
    "        for text in results:\n",
    "            f.write(text + \"\\n\")\n",
    "    \n",
    "    with open(\"Failed_papers.txt\", \"w\") as f:\n",
    "        f.write(\"Failed Downloads:\\n\")\n",
    "        for idx, pid in failed_papers[0]:\n",
    "            f.write(f\"{idx}: {pid}\\n\")\n",
    "        f.write(\"\\nFailed Extraction:\\n\")\n",
    "        for idx, pid in failed_papers[1]:\n",
    "            f.write(f\"{idx}: {pid}\\n\")\n",
    "        f.write(\"\\nFailed Introduction Detection:\\n\")\n",
    "        for idx, pid in failed_papers[2]:\n",
    "            f.write(f\"{idx}: {pid}\\n\")\n",
    "\n",
    "    return failed_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ids = 43000\n",
    "batch_size = int(num_ids / 15)  \n",
    "\n",
    "ranges = []\n",
    "\n",
    "start = 0\n",
    "while start < num_ids:\n",
    "    end = min(start + batch_size, num_ids)\n",
    "    ranges.append([start, end])\n",
    "    start = end\n",
    "\n",
    "ranges[-2][1] = ranges[-1][1]\n",
    "del ranges[-1]\n",
    "print(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_file = \"Node_IDs.txt\"  # File containing one ID per line\n",
    "\n",
    "output_file = \"graph-v3/Additional_Text.txt\"\n",
    "failed_papers = process_papers_parallel(id_file, ranges, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below concatenates the contents of a row from two files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_files(file1, file2, output_file):\n",
    "    with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2, open(output_file, 'w', encoding='utf-8') as out:\n",
    "        for line1, line2 in zip(f1, f2):\n",
    "            out.write(f\"{line1.strip()}. {line2.strip()}\\n\")\n",
    "\n",
    "# Example usage:\n",
    "concatenate_files(\"graph-v3/data-v2.txt\", \"graph-v3/Additional_Text.txt\", \"graph-v3/data-v3.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArXiv_Citation_Network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
