{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Libraries & Tools***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import requests\n",
    "import io\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm \n",
    "from datetime import datetime\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***PDF Extraction & Analysis***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Helper Functions & General Variables***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pdf(paper_id):\n",
    "    response = requests.get(f\"https://arxiv.org/pdf/{paper_id}\")\n",
    "    if response.status_code == 200:\n",
    "        return io.BytesIO(response.content)\n",
    "    else:\n",
    "        #raise Exception(f\"Failed to download PDF. Status code: {response.status_code}\")\n",
    "        return None \n",
    "\n",
    "def extract_text_from_pdf(pdf):\n",
    "    try:\n",
    "        doc = fitz.open(\"pdf\", pdf)\n",
    "        text = \"\"\n",
    "        for page in doc:\n",
    "            text += page.get_text(\"text\") + \"\\n\"\n",
    "        return text if text.strip() else None  # Return None if empty\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to extract text from {pdf}: {e}\")\n",
    "        return None  # Failed to extract text\n",
    "\n",
    "def extract_introduction(text, word_limit=250):\n",
    "    lines = text.split(\"\\n\")\n",
    "    introduction_found = False\n",
    "    extracted_text = []\n",
    "    word_count = 0\n",
    "    \n",
    "    section_pattern = re.compile(r'^\\s*(\\d+\\.?|[IVXLCDM]+\\.?|[A-Z]\\.?)\\s+(INTRODUCTION|Introduction)\\s*$')\n",
    "    \n",
    "    for line in lines:\n",
    "        if not introduction_found:\n",
    "            if section_pattern.match(line):\n",
    "                introduction_found = True\n",
    "        else:\n",
    "            #words = line.split()\n",
    "            words = re.findall(r\"\\b\\w+\\b\", line)\n",
    "            try:\n",
    "                if word_count + len(words) > word_limit:\n",
    "                    words = words[: word_limit - word_count]\n",
    "                    extracted_text.append(\" \".join(words))\n",
    "                    break\n",
    "                else:\n",
    "                    extracted_text.append(line)\n",
    "                    word_count += len(words)\n",
    "            except Exception:\n",
    "                break  \n",
    "    \n",
    "    cleaned_text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', \" \".join(extracted_text))\n",
    "    return cleaned_text.strip() if cleaned_text.strip() else None  # None if intro not found\n",
    "\n",
    "\n",
    "def update_output_file(output_file, successful_retries):\n",
    "    # Read the file into a list\n",
    "    with open(output_file, \"r\") as f:\n",
    "        lines = f.readlines()  # Read all lines\n",
    "\n",
    "    # Update the correct indices\n",
    "    for index, new_text in successful_retries:\n",
    "        if 0 <= index < len(lines):  # Ensure index is valid\n",
    "            lines[index] = new_text.strip() + \"\\n\" \n",
    "    \n",
    "    # Write back the updated lines\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "\n",
    "def concatenate_files(file1, file2, output_file):\n",
    "    with open(file1, 'r', encoding='utf-8') as f1, open(file2, 'r', encoding='utf-8') as f2, open(output_file, 'w', encoding='utf-8') as out:\n",
    "        for line1, line2 in zip(f1, f2):\n",
    "            if \"FAILED\" in line2:\n",
    "                out.write(f\"{line1.strip()}\\n\")\n",
    "            else:\n",
    "                out.write(f\"{line1.strip()}. {line2.strip()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_file = \"graph-v2/Node_IDs.txt\"  # File containing one ID per line\n",
    "\n",
    "output_file = \"graph-v2/Additional_Node_Content.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Serial***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process PDFs stored in directories\n",
    "def process_pdfs(pdf_dirs, results):\n",
    "    successes = 0\n",
    "    failures = []\n",
    "\n",
    "    for directory in pdf_dirs:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(directory, filename)\n",
    "\n",
    "                text = extract_text_from_pdf(pdf_path)\n",
    "                if text is None:\n",
    "                    print(f\"Skipping {filename}: Failed to extract text\")\n",
    "                    failures.append(filename.replace(\"_\", \"/\").replace(\".pdf\", \"\"))\n",
    "                    continue\n",
    "\n",
    "                intro_text = extract_introduction(text)\n",
    "                if intro_text is None:\n",
    "                    print(f\"Skipping {filename}: No introduction found\")\n",
    "                    failures.append(filename.replace(\"_\", \"/\").replace(\".pdf\", \"\"))\n",
    "                    continue\n",
    "\n",
    "                results[filename.replace(\"_\", \"/\").replace(\".pdf\", \"\")] = intro_text\n",
    "                successes += 1\n",
    "                #print(f\"✅ Extracted Introduction from {filename}\")\n",
    "\n",
    "    return results, successes, failures  # Dictionary { \"filename.pdf\": \"Introduction text\" }\n",
    "\n",
    "\n",
    "\n",
    "# Function to process individual papers\n",
    "def process_pdf(paper_id):\n",
    "    try:\n",
    "        pdf_stream = download_pdf(paper_id)\n",
    "        if not pdf_stream:\n",
    "            print(f\"{paper_id} failed to download\")\n",
    "            return None\n",
    "        \n",
    "        text = extract_text_from_pdf(pdf_stream)\n",
    "        if text is None: \n",
    "            print(f\"Failed to extract text from {paper_id}\")\n",
    "            return None\n",
    "        \n",
    "        introduction_text = extract_introduction(text)\n",
    "        if introduction_text is None:\n",
    "            print(f\"Failed to extract text from introduction section for {paper_id}\")\n",
    "            return None\n",
    "        \n",
    "        return introduction_text\n",
    "    except Exception as e:\n",
    "        return None #f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids = [\"0704.1274\", \"0704.1028\", \"0704.0954\", \"0704.1308\"] # For selected papers\n",
    "successful_retries = []\n",
    "\n",
    "for paper_id in paper_ids: # for idx, paper_id in failed_papers[0]:\n",
    "    intro_text = process_pdf(paper_id)\n",
    "    if intro_text:\n",
    "        print(f\"\\n==================================================\")\n",
    "        print(intro_text)\n",
    "        print(f\"==================================================\\n\")\n",
    "        #successful_retries.append((idx, intro_text)) # \"successful_retries\" will be used with \"failed_papers\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dirs = ['Papers 1', 'Papers 2', 'Papers 3', 'Papers 4', 'Papers 5', 'Papers 6']\n",
    "\n",
    "results = {}\n",
    "\n",
    "with open(id_file, \"r\") as f:\n",
    "    paper_ids = {line.strip() for line in f}  # Use a set for faster lookups\n",
    "\n",
    "for paper_id in paper_ids:\n",
    "    results[paper_id] = \"FAILED\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, successes, failures = process_pdfs(pdf_dirs, \"graph-v2/Node_IDs.txt\")\n",
    "\n",
    "print(f'Successes: {successes}. Failures: {len(failures)}. Node IDs: {len(paper_ids)}.')\n",
    "\n",
    "with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "    for filename, text in results.items():\n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Parallel***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_range(paper_ids, start, end):\n",
    "    results = [(i, \"FAILED\") for i in range(start, end)]\n",
    "    failed_papers = [[] for _ in range(3)]  # Track failures: [download, extraction, introduction]\n",
    "    \n",
    "    #for i, paper_id in enumerate(paper_ids, start=start):\n",
    "    for i, paper_id in enumerate(tqdm(paper_ids, total=len(paper_ids), desc=\"Processing Papers\"), start=start):\n",
    "\n",
    "        pdf_stream = download_pdf(paper_id)\n",
    "        if not pdf_stream:\n",
    "            failed_papers[0].append((i, paper_id))\n",
    "            continue\n",
    "        \n",
    "        text = extract_text_from_pdf(pdf_stream)\n",
    "        if text is None:\n",
    "            failed_papers[1].append((i, paper_id))\n",
    "            continue\n",
    "        \n",
    "        introduction_text = extract_introduction(text)\n",
    "        if introduction_text is None:\n",
    "            failed_papers[2].append((i, paper_id))\n",
    "            continue\n",
    "        \n",
    "        results[i - start] = (i, introduction_text)\n",
    "    \n",
    "    return results, failed_papers\n",
    "\n",
    "def process_papers_parallel(id_file, ranges, output_file, max_workers=5):\n",
    "    with open(id_file, \"r\") as f:\n",
    "        paper_ids = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    failed_papers = [[] for _ in range(3)]  # Track failures: [download, extraction, introduction]\n",
    "    results = [\"FAILED\"] * len(paper_ids)  # Initialize output list with \"FAILED\"\n",
    "    \n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_range = {executor.submit(process_range, paper_ids[r[0]:r[1]], r[0], r[1]): r for r in ranges}\n",
    "        \n",
    "        for future in future_to_range:\n",
    "            range_results, range_failures = future.result()\n",
    "            for i, text in range_results:\n",
    "                results[i] = text\n",
    "            for j in range(3):\n",
    "                failed_papers[j].extend(range_failures[j])\n",
    "    \n",
    "\n",
    "    return failed_papers, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 5259], [5259, 10518], [10518, 15777], [15777, 21036], [21036, 26295], [26295, 31554], [31554, 36813], [36813, 42072], [42072, 47331], [47331, 52596]]\n"
     ]
    }
   ],
   "source": [
    "# Create ranges\n",
    "num_ids = 52596\n",
    "batch_size = int(num_ids / 10)  \n",
    "\n",
    "ranges = []\n",
    "\n",
    "start = 0\n",
    "while start < num_ids:\n",
    "    end = min(start + batch_size, num_ids)\n",
    "    ranges.append([start, end])\n",
    "    start = end\n",
    "\n",
    "ranges[-2][1] = ranges[-1][1]\n",
    "del ranges[-1]\n",
    "print(ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_papers, results = process_papers_parallel(id_file, ranges, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, \"w\", encoding='utf-8') as f:\n",
    "    for text in results:\n",
    "        f.write(text + \"\\n\")\n",
    "    \n",
    "with open(\"graph-v2/Failed_papers.txt\", \"w\") as f:\n",
    "    f.write(\"Failed Downloads:\\n\")\n",
    "    for idx, pid in failed_papers[0]:\n",
    "        f.write(f\"{idx}: {pid}\\n\")\n",
    "    f.write(\"\\nFailed Extraction:\\n\")\n",
    "    for idx, pid in failed_papers[1]:\n",
    "        f.write(f\"{idx}: {pid}\\n\")\n",
    "    f.write(\"\\nFailed Introduction Detection:\\n\")\n",
    "    for idx, pid in failed_papers[2]:\n",
    "        f.write(f\"{idx}: {pid}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArXiv_Citation_Network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
